{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt \n",
    "%matplotlib inline \n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import datetime as DT \n",
    "import sqlite3\n",
    "import os.path\n",
    "from os import path\n",
    "import numpy as np \n",
    "import csv\n",
    "\n",
    "#maps capital -> country\n",
    "capitals = {'Sukhumi': 'Abkhazia', 'Kabul': 'Afghanistan', 'Episkopi Cantonment': 'Akrotiri and Dhekelia', 'Tirana': 'Albania', 'Algiers': 'Algeria', 'Pago Pago': 'American Samoa', 'Andorra la Vella': 'Andorra', 'Luanda': 'Angola', 'The Valley': 'Anguilla', \"St. John's\": 'Antigua and Barbuda', 'Buenos Aires': 'Argentina', 'Yerevan': 'Armenia', 'Oranjestad': 'Aruba', 'Georgetown': 'Guyana', 'Canberra': 'Australia', 'Vienna': 'Austria', 'Baku': 'Azerbaijan', 'Nassau': 'Bahamas', 'Manama': 'Bahrain', 'Dhaka': 'Bangladesh', 'Bridgetown': 'Barbados', 'Minsk': 'Belarus', 'Brussels': 'Belgium', 'Belmopan': 'Belize', 'Porto-Novo': 'Benin', 'Hamilton': 'Bermuda', 'Thimphu': 'Bhutan', 'Sucre': 'Bolivia', 'La Paz': 'Bolivia', 'Sarajevo': 'Bosnia and Herzegovina', 'Gaborone': 'Botswana', 'Brasília': 'Brazil', 'Road Town': 'British Virgin Islands', 'Bandar Seri Begawan': 'Brunei', 'Sofia': 'Bulgaria', 'Ouagadougou': 'Burkina Faso', 'Bujumbura': 'Burundi', 'Phnom Penh': 'Cambodia', 'Yaoundé': 'Cameroon', 'Ottawa': 'Canada', 'Praia': 'Cape Verde', 'George Town': 'Cayman Islands', 'Bangui': 'Central African Republic', \"N'Djamena\": 'Chad', 'Santiago': 'Chile', 'Beijing': 'China', 'Flying Fish Cove': 'Christmas Island', 'West Island': 'Cocos (Keeling) Islands', 'Bogotá': 'Colombia', 'Moroni': 'Comoros', 'Avarua': 'Cook Islands', 'San José': 'Costa Rica', 'Zagreb': 'Croatia', 'Havana': 'Cuba', 'Willemstad': 'Curaçao', 'Nicosia': 'Northern Cyprus', 'Prague': 'Czech Republic', 'Yamoussoukro': \"Côte d'Ivoire\", 'Kinshasa': 'Democratic Republic of the Congo', 'Copenhagen': 'Denmark', 'Djibouti': 'Djibouti', 'Roseau': 'Dominica', 'Santo Domingo': 'Dominican Republic', 'Dili': 'East Timor (Timor-Leste)', 'Hanga Roa': 'Easter Island', 'Quito': 'Ecuador', 'Cairo': 'Egypt', 'San Salvador': 'El Salvador', 'Malabo': 'Equatorial Guinea', 'Asmara': 'Eritrea', 'Tallinn': 'Estonia', 'Addis Ababa': 'Ethiopia', 'Stanley': 'Falkland Islands', 'Tórshavn': 'Faroe Islands', 'Palikir': 'Federated States of Micronesia', 'Suva': 'Fiji', 'Helsinki': 'Finland', 'Paris': 'France', 'Cayenne': 'French Guiana', 'Papeete': 'French Polynesia', 'Libreville': 'Gabon', 'Banjul': 'Gambia', 'Tbilisi': 'Georgia', 'Berlin': 'Germany', 'Accra': 'Ghana', 'Gibraltar': 'Gibraltar', 'Athens': 'Greece', 'Nuuk': 'Greenland', \"St. George's\": 'Grenada', 'Hagåtña': 'Guam', 'Guatemala City': 'Guatemala', 'St. Peter Port': 'Guernsey', 'Conakry': 'Guinea', 'Bissau': 'Guinea-Bissau', 'Port-au-Prince': 'Haiti', 'Tegucigalpa': 'Honduras', 'Budapest': 'Hungary', 'Reykjavík': 'Iceland', 'New Delhi': 'India', 'Jakarta': 'Indonesia', 'Tehran': 'Iran', 'Baghdad': 'Iraq', 'Dublin': 'Ireland', 'Douglas': 'Isle of Man', 'Jerusalem': 'Palestine', 'Rome': 'Italy', 'Kingston': 'Norfolk Island', 'Tokyo': 'Japan', 'St. Helier': 'Jersey', 'Amman': 'Jordan', 'Astana': 'Kazakhstan', 'Nairobi': 'Kenya', 'Tarawa': 'Kiribati', 'Pristina': 'Kosovo', 'Kuwait City': 'Kuwait', 'Bishkek': 'Kyrgyzstan', 'Vientiane': 'Laos', 'Riga': 'Latvia', 'Beirut': 'Lebanon', 'Maseru': 'Lesotho', 'Monrovia': 'Liberia', 'Tripoli': 'Libya', 'Vaduz': 'Liechtenstein', 'Vilnius': 'Lithuania', 'Luxembourg': 'Luxembourg', 'Skopje': 'Macedonia', 'Antananarivo': 'Madagascar', 'Lilongwe': 'Malawi', 'Kuala Lumpur': 'Malaysia', 'Malé': 'Maldives', 'Bamako': 'Mali', 'Valletta': 'Malta', 'Majuro': 'Marshall Islands', 'Nouakchott': 'Mauritania', 'Port Louis': 'Mauritius', 'Mexico City': 'Mexico', 'Chisinau': 'Moldova', 'Monaco': 'Monaco', 'Ulaanbaatar': 'Mongolia', 'Podgorica': 'Montenegro', 'Plymouth': 'Montserrat', 'Rabat': 'Morocco', 'Maputo': 'Mozambique', 'Naypyidaw': 'Myanmar', 'Stepanakert': 'Nagorno-Karabakh Republic', 'Windhoek': 'Namibia', 'Yaren': 'Nauru', 'Kathmandu': 'Nepal', 'Amsterdam': 'Netherlands', 'Nouméa': 'New Caledonia', 'Wellington': 'New Zealand', 'Managua': 'Nicaragua', 'Niamey': 'Niger', 'Abuja': 'Nigeria', 'Alofi': 'Niue', 'Pyongyang': 'North Korea', 'Belfast': 'United Kingdom Northern Ireland', 'Saipan': 'Northern Mariana Islands', 'Oslo': 'Norway', 'Muscat': 'Oman', 'Islamabad': 'Pakistan', 'Ngerulmud': 'Palau', 'Panama City': 'Panama', 'Port Moresby': 'Papua New Guinea', 'Asunción': 'Paraguay', 'Lima': 'Peru', 'Manila': 'Philippines', 'Adamstown': 'Pitcairn Islands', 'Warsaw': 'Poland', 'Lisbon': 'Portugal', 'San Juan': 'Puerto Rico', 'Doha': 'Qatar', 'Taipei': 'Republic of China (Taiwan)', 'Brazzaville': 'Republic of the Congo', 'Bucharest': 'Romania', 'Moscow': 'Russia', 'Kigali': 'Rwanda', 'Gustavia': 'Saint Barthélemy', 'Jamestown': 'Saint Helena', 'Basseterre': 'Saint Kitts and Nevis', 'Castries': 'Saint Lucia', 'Marigot': 'Saint Martin', 'St. Pierre': 'Saint Pierre and Miquelon', 'Kingstown': 'Saint Vincent and the Grenadines', 'Apia': 'Samoa', 'San Marino': 'San Marino', 'Riyadh': 'Saudi Arabia', 'Edinburgh': 'Scotland', 'Dakar': 'Senegal', 'Belgrade': 'Serbia', 'Victoria': 'Seychelles', 'Freetown': 'Sierra Leone', 'Singapore': 'Singapore', 'Philipsburg': 'Sint Maarten', 'Bratislava': 'Slovakia', 'Ljubljana': 'Slovenia', 'Honiara': 'Solomon Islands', 'Mogadishu': 'Somalia', 'Hargeisa': 'Somaliland', 'Pretoria': 'South Africa', 'Grytviken': 'South Georgia and the South Sandwich Islands', 'Seoul': 'South Korea', 'Tskhinvali': 'South Ossetia', 'Juba': 'South Sudan South Sudan', 'Madrid': 'Spain', 'Sri Jayawardenapura Kotte': 'Sri Lanka', 'Khartoum': 'Sudan', 'Paramaribo': 'Suriname', 'Mbabane': 'Swaziland', 'Stockholm': 'Sweden', 'Bern': 'Switzerland', 'Damascus': 'Syria', 'São Tomé': 'São Tomé and Príncipe', 'Dushanbe': 'Tajikistan', 'Dodoma': 'Tanzania', 'Bangkok': 'Thailand', 'Lomé': 'Togo', 'Nukuʻalofa': 'Tonga', 'Tiraspol': 'Transnistria', 'Port of Spain': 'Trinidad and Tobago', 'Edinburgh of the Seven Seas': 'Tristan da Cunha', 'Tunis': 'Tunisia', 'Ankara': 'Turkey', 'Ashgabat': 'Turkmenistan', 'Cockburn Town': 'Turks and Caicos Islands', 'Funafuti': 'Tuvalu', 'Kampala': 'Uganda', 'Kiev': 'Ukraine', 'Abu Dhabi': 'United Arab Emirates', 'London': 'United Kingdom; England', 'Washington, D.C.': 'United States', 'Charlotte Amalie': 'United States Virgin Islands', 'Montevideo': 'Uruguay', 'Tashkent': 'Uzbekistan', 'Port Vila': 'Vanuatu', 'Vatican City': 'Vatican City', 'Caracas': 'Venezuela', 'Hanoi': 'Vietnam', 'Cardiff': 'Wales', 'Mata-Utu': 'Wallis and Futuna', 'El Aaiún': 'Western Sahara', 'Sanaá': 'Yemen', 'Lusaka': 'Zambia', 'Harare': 'Zimbabwe'}\n",
    "\n",
    "#get list of cities \n",
    "cities = list(capitals.keys()) \n",
    "\n",
    "#list of 73 most influential countries in the world  \n",
    "influential_countries = ['Switzerland', 'Canada', 'Japan', 'Germany', 'Australia', 'United Kingdom', 'United States', 'Sweden', 'Netherlands', 'Norway', 'New Zealand', 'France', 'Denmark', 'Finland', 'China', 'Singapore', 'Italy', 'Austria', 'Spain', 'South Korea', 'Luxembourg', 'United Arab Emirates', 'Russia', 'Portugal', 'India', 'Thailand', 'Greece', 'Brazil', 'Israel', 'Qatar', 'Saudi Arabia', 'Malaysia', 'Mexico', 'Poland', 'Turkey', 'Egypt', 'Czech Republic', 'Costa Rica', 'South Africa', 'Morocco', 'Indonesia', 'Argentina', 'Vietnam', 'Croatia', 'Philippines', 'Chile', 'Peru', 'Sri Lanka', 'Dominican Republic', 'Panama', 'Colombia', 'Slovakia', 'Kenya', 'Romania', 'Estonia', 'Slovenia', 'Myanmar', 'Bulgaria', 'Lithuania', 'Bolivia', 'Azerbaijan', 'Latvia', 'Ecuador', 'Jordan', 'Guatemala', 'Kazakhstan', 'Ghana', 'Ukraine', 'Tunisia', 'Belarus', 'Oman', 'Serbia', 'Lebanon']\n",
    "\n",
    "#classification I made with Continent: Country. Just used a bunch of random techniques to get this. \n",
    "classif = {'Algeria': 'Africa', 'Angola': 'Africa', 'Benin': 'Africa', 'Botswana': 'Africa', 'Burkina': 'Africa', 'Burundi': 'Africa', 'Cameroon': 'Africa', 'Cape Verde': 'Africa', 'Central African Republic': 'Africa', 'Chad': 'Africa', 'Comoros': 'Africa', 'Congo': 'Africa', 'Djibouti': 'Africa', 'Egypt': 'Africa', 'Equatorial Guinea': 'Africa', 'Eritrea': 'Africa', 'Ethiopia': 'Africa', 'Gabon': 'Africa', 'Gambia': 'Africa', 'Ghana': 'Africa', 'Guinea': 'Africa', 'Guinea-Bissau': 'Africa', 'Ivory Coast': 'Africa', 'Kenya': 'Africa', 'Lesotho': 'Africa', 'Liberia': 'Africa', 'Libya': 'Africa', 'Madagascar': 'Africa', 'Malawi': 'Africa', 'Mali': 'Africa', 'Mauritania': 'Africa', 'Mauritius': 'Africa', 'Morocco': 'Africa', 'Mozambique': 'Africa', 'Namibia': 'Africa', 'Niger': 'Africa', 'Nigeria': 'Africa', 'Rwanda': 'Africa', 'Sao Tome and Principe': 'Africa', 'Senegal': 'Africa', 'Seychelles': 'Africa', 'Sierra Leone': 'Africa', 'Somalia': 'Africa', 'South Africa': 'Africa', 'South Sudan': 'Africa', 'Sudan': 'Africa', 'Swaziland': 'Africa', 'Tanzania': 'Africa', 'Togo': 'Africa', 'Tunisia': 'Africa', 'Uganda': 'Africa', 'Zambia': 'Africa', 'Zimbabwe': 'Africa', 'Afghanistan': 'Asia', 'Bahrain': 'Asia', 'Bangladesh': 'Asia', 'Bhutan': 'Asia', 'Brunei': 'Asia', 'Myanmar': 'Asia', 'Cambodia': 'Asia', 'China': 'Asia', 'East Timor': 'Asia', 'India': 'Asia', 'Indonesia': 'Asia', 'Iran': 'Asia', 'Iraq': 'Asia', 'Israel': 'Asia', 'Japan': 'Asia', 'Jordan': 'Asia', 'Kazakhstan': 'Asia', 'South Korea': 'Asia', 'Kuwait': 'Asia', 'Kyrgyzstan': 'Asia', 'Laos': 'Asia', 'Lebanon': 'Asia', 'Malaysia': 'Asia', 'Maldives': 'Asia', 'Mongolia': 'Asia', 'Nepal': 'Asia', 'Oman': 'Asia', 'Pakistan': 'Asia', 'Philippines': 'Asia', 'Qatar': 'Asia', 'Russia': 'Asia', 'Saudi Arabia': 'Asia', 'Singapore': 'Asia', 'Sri Lanka': 'Asia', 'Syria': 'Asia', 'Tajikistan': 'Asia', 'Thailand': 'Asia', 'Turkey': 'Asia', 'Turkmenistan': 'Asia', 'United Arab Emirates': 'Asia', 'Uzbekistan': 'Asia', 'Vietnam': 'Asia', 'Yemen': 'Asia', 'Albania': 'Europe', 'Andorra': 'Europe', 'Armenia': 'Europe', 'Austria': 'Europe', 'Azerbaijan': 'Europe', 'Belarus': 'Europe', 'Belgium': 'Europe', 'Bosnia and Herzegovina': 'Europe', 'Bulgaria': 'Europe', 'Croatia': 'Europe', 'Cyprus': 'Europe', 'Czech Republic': 'Europe', 'Denmark': 'Europe', 'Estonia': 'Europe', 'Finland': 'Europe', 'France': 'Europe', 'Georgia': 'Europe', 'Germany': 'Europe', 'Greece': 'Europe', 'Hungary': 'Europe', 'Iceland': 'Europe', 'Ireland': 'Europe', 'Italy': 'Europe', 'Latvia': 'Europe', 'Liechtenstein': 'Europe', 'Lithuania': 'Europe', 'Luxembourg': 'Europe', 'Macedonia': 'Europe', 'Malta': 'Europe', 'Moldova': 'Europe', 'Monaco': 'Europe', 'Montenegro': 'Europe', 'Netherlands': 'Europe', 'Norway': 'Europe', 'Poland': 'Europe', 'Portugal': 'Europe', 'Romania': 'Europe', 'San Marino': 'Europe', 'Serbia': 'Europe', 'Slovakia': 'Europe', 'Slovenia': 'Europe', 'Spain': 'Europe', 'Sweden': 'Europe', 'Switzerland': 'Europe', 'Ukraine': 'Europe', 'United Kingdom': 'Europe', 'Vatican City': 'Europe', 'Antigua and Barbuda': 'North America', 'Bahamas': 'North America', 'Barbados': 'North America', 'Belize': 'North America', 'Canada': 'North America', 'Costa Rica': 'North America', 'Cuba': 'North America', 'Dominica': 'North America', 'Dominican Republic': 'North America', 'El Salvador': 'North America', 'Grenada': 'North America', 'Guatemala': 'North America', 'Haiti': 'North America', 'Honduras': 'North America', 'Jamaica': 'North America', 'Mexico': 'North America', 'Nicaragua': 'North America', 'Panama': 'North America', 'Saint Kitts and Nevis': 'North America', 'Saint Lucia': 'North America', 'Saint Vincent and the Grenadines': 'North America', 'Trinidad and Tobago': 'North America', 'United States': 'North America', 'Australia': 'Oceania', 'Fiji': 'Oceania', 'Kiribati': 'Oceania', 'Marshall Islands': 'Oceania', 'Micronesia': 'Oceania', 'Nauru': 'Oceania', 'New Zealand': 'Oceania', 'Palau': 'Oceania', 'Papua New Guinea': 'Oceania', 'Samoa': 'Oceania', 'Solomon Islands': 'Oceania', 'Tonga': 'Oceania', 'Tuvalu': 'Oceania', 'Vanuatu': 'Oceania', 'Argentina': 'South America', 'Bolivia': 'South America', 'Brazil': 'South America', 'Chile': 'South America', 'Colombia': 'South America', 'Ecuador': 'South America', 'Guyana': 'South America', 'Paraguay': 'South America', 'Peru': 'South America', 'Suriname': 'South America', 'Uruguay': 'South America', 'Venezuela': 'South America'}\n",
    "\n",
    "#todays date \n",
    "today = str(DT.date.today()).split('date')[0]\n",
    "\n",
    "#top 20 \n",
    "top20 = ['Switzerland','Canada','Japan','Germany','Australia','United Kingdom','United States','Sweden','Netherlands','Norway','New Zealand','France','Denmark','Finland','China','Singapore','Italy','Austria','Spain','South Korea']\n",
    "\n",
    "\n",
    "#top20 list for covid-19 cases class \n",
    "top20forfinalcountrycases = ['Switzerland','Canada','Japan','Germany','Australia','UK','USA','Sweden','Netherlands','Norway','New Zealand','France','Denmark','Finland','China','Singapore','Italy','Austria','Spain','S. Korea']\n",
    "\n",
    "#create 'data' database \n",
    "if path.exists('data.db') is False: \n",
    "    conn = sqlite3.connect('data.db')\n",
    "    conn.close() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weather Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('2020-03-29', 'Switzerland', 37.04)\n",
      "('2020-03-29', 'Canada', 68.0)\n",
      "('2020-03-29', 'Japan', 35.96)\n",
      "('2020-03-29', 'Germany', 32.36)\n",
      "('2020-03-29', 'Australia', 60.08)\n",
      "('2020-03-29', 'United Kingdom', 75.02)\n",
      "('2020-03-29', 'United States', 78.98)\n",
      "('2020-03-29', 'Sweden', 33.08)\n",
      "('2020-03-29', 'Netherlands', 35.96)\n",
      "('2020-03-29', 'Norway', 37.04)\n",
      "('2020-03-29', 'New Zealand', 77.72)\n",
      "('2020-03-29', 'France', 42.08)\n",
      "('2020-03-29', 'Denmark', 35.96)\n",
      "('2020-03-29', 'Finland', 28.94)\n",
      "('2020-03-29', 'China', 35.6)\n",
      "('2020-03-29', 'Singapore', 80.6)\n",
      "('2020-03-29', 'Italy', 53.96)\n",
      "('2020-03-29', 'Austria', 84.38000000000001)\n",
      "('2020-03-29', 'Spain', 68.0)\n",
      "('2020-03-29', 'South Korea', 35.6)\n",
      "('2020-03-30', 'Switzerland', 44.96)\n",
      "('2020-03-30', 'Canada', 64.04)\n",
      "('2020-03-30', 'Japan', 44.96)\n",
      "('2020-03-30', 'Germany', 37.04)\n",
      "('2020-03-30', 'Australia', 62.059999999999995)\n",
      "('2020-03-30', 'United Kingdom', 57.2)\n",
      "('2020-03-30', 'United States', 82.4)\n",
      "('2020-03-30', 'Sweden', 30.92)\n",
      "('2020-03-30', 'Netherlands', 45.68)\n",
      "('2020-03-30', 'Norway', 55.04)\n",
      "('2020-03-30', 'New Zealand', 83.48)\n",
      "('2020-03-30', 'France', 50.0)\n",
      "('2020-03-30', 'Denmark', 42.980000000000004)\n",
      "('2020-03-30', 'Finland', 33.08)\n",
      "('2020-03-30', 'China', 53.6)\n",
      "('2020-03-30', 'Singapore', 84.2)\n",
      "('2020-03-30', 'Italy', 60.980000000000004)\n",
      "('2020-03-30', 'Austria', 75.02)\n",
      "('2020-03-30', 'Spain', 62.6)\n",
      "('2020-03-30', 'South Korea', 39.2)\n",
      "('2020-04-01', 'Switzerland', 48.02)\n",
      "('2020-04-01', 'Canada', 60.08)\n",
      "('2020-04-01', 'Japan', 57.92)\n",
      "('2020-04-01', 'Germany', 43.879999999999995)\n",
      "('2020-04-01', 'Australia', 62.059999999999995)\n",
      "('2020-04-01', 'United Kingdom', 53.06)\n",
      "('2020-04-01', 'United States', 80.96)\n",
      "('2020-04-01', 'Sweden', 48.92)\n",
      "('2020-04-01', 'Netherlands', 45.14)\n",
      "('2020-04-01', 'Norway', 55.94)\n",
      "('2020-04-01', 'New Zealand', 85.28)\n",
      "('2020-04-01', 'France', 57.02)\n",
      "('2020-04-01', 'Denmark', 46.04)\n",
      "('2020-04-01', 'Finland', 44.06)\n",
      "('2020-04-01', 'China', 48.2)\n",
      "('2020-04-01', 'Singapore', 84.2)\n",
      "('2020-04-01', 'Italy', 53.06)\n",
      "('2020-04-01', 'Austria', 54.5)\n",
      "('2020-04-01', 'Spain', 59.0)\n",
      "('2020-04-01', 'South Korea', 41.0)\n",
      "('2020-04-05', 'Switzerland', 50.0)\n",
      "('2020-04-05', 'Canada', 53.06)\n",
      "('2020-04-05', 'Japan', 57.2)\n",
      "('2020-04-05', 'Germany', 42.08)\n",
      "('2020-04-05', 'Australia', 60.980000000000004)\n",
      "('2020-04-05', 'United Kingdom', 59.0)\n",
      "('2020-04-05', 'United States', 78.98)\n",
      "('2020-04-05', 'Sweden', 37.04)\n",
      "('2020-04-05', 'Netherlands', 48.92)\n",
      "('2020-04-05', 'Norway', 39.02)\n",
      "('2020-04-05', 'New Zealand', 78.44)\n",
      "('2020-04-05', 'France', 59.0)\n",
      "('2020-04-05', 'Denmark', 44.06)\n",
      "('2020-04-05', 'Finland', 35.96)\n",
      "('2020-04-05', 'China', 59.0)\n",
      "('2020-04-05', 'Singapore', 84.2)\n",
      "('2020-04-05', 'Italy', 55.04)\n",
      "('2020-04-05', 'Austria', 69.25999999999999)\n",
      "('2020-04-05', 'Spain', 57.2)\n",
      "('2020-04-05', 'South Korea', 51.8)\n",
      "('2020-04-07', 'Switzerland', 53.96)\n",
      "('2020-04-07', 'Canada', 78.08)\n",
      "('2020-04-07', 'Japan', 44.06)\n",
      "('2020-04-07', 'Germany', 49.28)\n",
      "('2020-04-07', 'Australia', 55.04)\n",
      "('2020-04-07', 'United Kingdom', 78.8)\n",
      "('2020-04-07', 'United States', 82.4)\n",
      "('2020-04-07', 'Sweden', 39.02)\n",
      "('2020-04-07', 'Netherlands', 52.16)\n",
      "('2020-04-07', 'Norway', 44.96)\n",
      "('2020-04-07', 'New Zealand', 78.8)\n",
      "('2020-04-07', 'France', 62.059999999999995)\n",
      "('2020-04-07', 'Denmark', 44.6)\n",
      "('2020-04-07', 'Finland', 44.06)\n",
      "('2020-04-07', 'China', 35.6)\n",
      "('2020-04-07', 'Singapore', 80.6)\n",
      "('2020-04-07', 'Italy', 57.02)\n",
      "('2020-04-07', 'Austria', 76.64)\n",
      "('2020-04-07', 'Spain', 68.0)\n",
      "('2020-04-07', 'South Korea', 35.6)\n",
      "('2020-04-11', 'Switzerland', 46.94)\n",
      "('2020-04-11', 'Canada', 71.06)\n",
      "('2020-04-11', 'Japan', 44.96)\n",
      "('2020-04-11', 'Germany', 44.06)\n",
      "('2020-04-11', 'Australia', 48.92)\n",
      "('2020-04-11', 'United Kingdom', 57.2)\n",
      "('2020-04-11', 'United States', 82.4)\n",
      "('2020-04-11', 'Sweden', 41.0)\n",
      "('2020-04-11', 'Netherlands', 49.28)\n",
      "('2020-04-11', 'Norway', 41.0)\n",
      "('2020-04-11', 'New Zealand', 78.8)\n",
      "('2020-04-11', 'France', 64.94)\n",
      "('2020-04-11', 'Denmark', 46.94)\n",
      "('2020-04-11', 'Finland', 35.06)\n",
      "('2020-04-11', 'China', 57.2)\n",
      "('2020-04-11', 'Singapore', 80.6)\n",
      "('2020-04-11', 'Italy', 57.02)\n",
      "('2020-04-11', 'Austria', 72.14)\n",
      "('2020-04-11', 'Spain', 60.8)\n",
      "('2020-04-11', 'South Korea', 46.4)\n",
      "('2020-04-17', 'Switzerland', 48.92)\n",
      "('2020-04-17', 'Canada', 37.04)\n",
      "('2020-04-17', 'Japan', 57.2)\n",
      "('2020-04-17', 'Germany', 41.72)\n",
      "('2020-04-17', 'Australia', 60.980000000000004)\n",
      "('2020-04-17', 'United Kingdom', 53.06)\n",
      "('2020-04-17', 'United States', 78.8)\n",
      "('2020-04-17', 'Sweden', 35.06)\n",
      "('2020-04-17', 'Netherlands', 43.519999999999996)\n",
      "('2020-04-17', 'Norway', 37.04)\n",
      "('2020-04-17', 'New Zealand', 77.36)\n",
      "('2020-04-17', 'France', 60.980000000000004)\n",
      "('2020-04-17', 'Denmark', 42.980000000000004)\n",
      "('2020-04-17', 'Finland', 35.96)\n",
      "('2020-04-17', 'China', 66.2)\n",
      "('2020-04-17', 'Singapore', 86.0)\n",
      "('2020-04-17', 'Italy', 57.02)\n",
      "('2020-04-17', 'Austria', 68.18)\n",
      "('2020-04-17', 'Spain', 57.2)\n",
      "('2020-04-17', 'South Korea', 57.2)\n",
      "('2020-04-18', 'Switzerland', 53.06)\n",
      "('2020-04-18', 'Canada', 50.0)\n",
      "('2020-04-18', 'Japan', 48.92)\n",
      "('2020-04-18', 'Germany', 58.64)\n",
      "('2020-04-18', 'Australia', 42.980000000000004)\n",
      "('2020-04-18', 'United Kingdom', 46.4)\n",
      "('2020-04-18', 'United States', 86.0)\n",
      "('2020-04-18', 'Sweden', 48.02)\n",
      "('2020-04-18', 'Netherlands', 57.92)\n",
      "('2020-04-18', 'Norway', 57.92)\n",
      "('2020-04-18', 'New Zealand', 90.14)\n",
      "('2020-04-18', 'France', 66.92)\n",
      "('2020-04-18', 'Denmark', 51.08)\n",
      "('2020-04-18', 'Finland', 46.94)\n",
      "('2020-04-18', 'China', 59.0)\n",
      "('2020-04-18', 'Singapore', 82.4)\n",
      "('2020-04-18', 'Italy', 64.04)\n",
      "('2020-04-18', 'Austria', 73.75999999999999)\n",
      "('2020-04-18', 'Spain', 64.4)\n",
      "('2020-04-18', 'South Korea', 50.0)\n"
     ]
    }
   ],
   "source": [
    "class WeatherData: \n",
    "    \n",
    "    def __init__(self): \n",
    "        \"\"\"\n",
    "        Initializes each Weather Data object. Creates city_urls attribute which contains all request urls for all capital cities around the world. \n",
    "        \"\"\"\n",
    "        city_urls = []\n",
    "        for city in cities: \n",
    "            city_urls.append(self.create_request_url_weather(city))\n",
    "            \n",
    "        #save as attribute \n",
    "        self.city_urls = city_urls\n",
    "     \n",
    "        \n",
    "    #API Key #1: 3283190565ac48b9bfff13e1b9b319c1\n",
    "    #API Key #2: eedbb9c044e94669ba9bc59b4e1e7a6e\n",
    "    def create_request_url_weather(self, city):  \n",
    "        \"\"\"\n",
    "        Takes in a city, returns request URL. \n",
    "        \"\"\"\n",
    "        url = 'https://api.weatherbit.io/v2.0/current?city='+ city + '&key=' + 'eedbb9c044e94669ba9bc59b4e1e7a6e'\n",
    "        return url \n",
    "\n",
    "\n",
    "    def fit_weather_list(self, country_weather): \n",
    "        \"\"\"\n",
    "        Take in a dictionary 'country_weather'. Returns a dictionary with fitted list. \n",
    "        \"\"\"\n",
    "        #make the following edits: #'United Kingdom; England' -> United Kingdom, Palestine -> Israel, United States Virgin Islands -> United States \n",
    "        deletelist = ['United Kingdom; England', 'Palestine', 'United States Virgin Islands']\n",
    "        countryfixlist = ['United Kingdom', 'Israel', 'United States'] \n",
    "        datafixlist = []\n",
    "        returndict = {}\n",
    "\n",
    "        #grab corresponding values for the keys that you are about to delete, create datafixlist \n",
    "        for country in deletelist:\n",
    "            try: \n",
    "                datafixlist.append(country_weather[country]) \n",
    "            except: \n",
    "                continue \n",
    "        #delete all wrongly named countries \n",
    "        for country in deletelist: \n",
    "            try: \n",
    "                del country_weather[country]\n",
    "            except: \n",
    "                continue \n",
    "\n",
    "        #reappend the correct key value pairs with correct country nomenclature \n",
    "        for i in range(len(datafixlist)):\n",
    "            try: \n",
    "                country_weather[countryfixlist[i]] = datafixlist[i]\n",
    "            except: \n",
    "                continue \n",
    "        #remove all countries not considered influential \n",
    "        for country in influential_countries: \n",
    "            try: \n",
    "                returndict[country] = country_weather[country]\n",
    "            except: \n",
    "                continue \n",
    "        return returndict \n",
    "\n",
    "    def get_weather_data(self): \n",
    "        \"\"\"\n",
    "        Gets weather data using the list of urls, returns a dictionary called country_weather. \n",
    "        \"\"\"\n",
    "        country_weather = {}\n",
    "        for url in self.city_urls:\n",
    "            try: \n",
    "                #requests returns a response object, convert response content to dictionary \n",
    "                data = requests.get(url).json()\n",
    "                #grab the city and grab the corresponding country from capitals[dict]\n",
    "                city = url.split('=')[1].split('&')[0].strip()\n",
    "                country = capitals[city]\n",
    "                weather = float(data['data'][0]['temp']) \n",
    "                country_weather[country] = weather\n",
    "            except: \n",
    "                continue\n",
    "            \n",
    "        country_weather = self.fit_weather_list(country_weather)\n",
    "        return country_weather\n",
    "    \n",
    "    \n",
    "    def create_insert_weather_tuple(self, country_weather):\n",
    "        \"\"\"\n",
    "        Takes in a dictionary, returns a tuple of values with only 20 top most influential countries to insert into database. \n",
    "        \"\"\"\n",
    "        top20tuplefinal = []\n",
    "        insertweathertuple = []\n",
    "        \n",
    "        \n",
    "        #create a list from the top 20 tuple     \n",
    "        top20tuple = list(country_weather.items()) \n",
    "\n",
    "        #create a final list of tuples that ONLY CONTAIN TOP 20 countries chosen, convert -> dataframe \n",
    "        for i in range(len(top20tuple)):\n",
    "            if top20tuple[i][0] in top20: \n",
    "                top20tuplefinal.append(top20tuple[i])\n",
    "\n",
    "        for tup in top20tuplefinal: \n",
    "            insertweathertuple.append((today, tup[0], tup[1])) \n",
    "        return insertweathertuple\n",
    "    \n",
    "    #run everytime you want to create/append \n",
    "    def create_and_append_weather(self, insertweathertuple): \n",
    "        \"\"\"\n",
    "        Grab data and append data to country_weather table in data database. No return value.  \n",
    "        \"\"\"\n",
    "        #connect/create database\n",
    "        conn = sqlite3.connect('data.db')\n",
    "        conn.execute('''CREATE TABLE if not exists country_weather (date text, country text, Fahrenheit FLOAT)''')\n",
    "        conn.executemany('INSERT INTO country_weather VALUES (?,?,?)', insertweathertuple)\n",
    "        #print output \n",
    "        for row in conn.execute('SELECT * FROM country_weather'):\n",
    "            print(row)\n",
    "        conn.commit()\n",
    "        conn.close() \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def convert_to_fahrenheit(dictionary):\n",
    "    \"\"\"\n",
    "    Convert a dictionary from celsius to fahrenheit by modifying the values. Returns dictionary with all values in fahrenheit. \n",
    "    \"\"\"\n",
    "    for key in dictionary.keys(): \n",
    "        dictionary[key] = dictionary[key]*9/5 + 32 \n",
    "    return dictionary\n",
    "\n",
    "\n",
    "#UN-COMMENT BELOW TO ADD DATA. RUN THIS ONCE A DAY! FOR 5 days!\n",
    "weatherobj = WeatherData() \n",
    "country_weather = weatherobj.get_weather_data()\n",
    "country_weather = convert_to_fahrenheit(country_weather)\n",
    "final_country_weather = country_weather\n",
    "weatherobj.create_and_append_weather(weatherobj.create_insert_weather_tuple(country_weather))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COVID-19 Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('2020-03-29', 14076.0, 14076.0, 1530.0, 1148.0, 33.0, 12282.0, 'Switzerland')\n",
      "('2020-03-29', 5655.0, 5655.0, 396.0, 898.0, 5.0, 5199.0, 'Canada')\n",
      "('2020-03-29', 1693.0, 1693.0, 404.0, 194.0, 3.0, 1237.0, 'Japan')\n",
      "('2020-03-29', 57695.0, 57695.0, 8481.0, 6824.0, 82.0, 48781.0, 'Germany')\n",
      "('2020-03-29', 3635.0, 3635.0, 170.0, 257.0, 1.0, 3451.0, 'Australia')\n",
      "('2020-03-29', 17089.0, 17089.0, 135.0, 2546.0, 260.0, 15935.0, 'United Kingdom')\n",
      "('2020-03-29', 123313.0, 123313.0, 3231.0, 19187.0, 515.0, 117871.0, 'United States')\n",
      "('2020-03-29', 3447.0, 3447.0, 16.0, 378.0, None, None, None)\n",
      "('2020-03-29', 9762.0, 9762.0, 3.0, 1159.0, 93.0, 9120.0, 'Netherlands')\n",
      "('2020-03-29', 4015.0, 4015.0, 7.0, 244.0, 4.0, 3985.0, 'Norway')\n",
      "('2020-03-29', 451.0, 451.0, 50.0, 83.0, None, None, None)\n",
      "('2020-03-29', 37575.0, 37575.0, 5700.0, 4611.0, 319.0, 29561.0, 'France')\n",
      "('2020-03-29', 2201.0, 2201.0, 1.0, 155.0, 13.0, 2135.0, 'Denmark')\n",
      "('2020-03-29', 1167.0, 1167.0, 10.0, 126.0, 2.0, 1148.0, 'Finland')\n",
      "('2020-03-29', 81394.0, 81394.0, 74971.0, None, None, None, None)\n",
      "('2020-03-29', 802.0, 802.0, 198.0, 70.0, None, None, None)\n",
      "('2020-03-29', 92472.0, 92472.0, 12384.0, 5974.0, 889.0, 70065.0, 'Italy')\n",
      "('2020-03-29', 8271.0, 8271.0, 225.0, 574.0, 10.0, 7978.0, 'Austria')\n",
      "('2020-03-29', 73235.0, 73235.0, 12285.0, 7516.0, 844.0, 54968.0, 'Spain')\n",
      "('2020-03-29', 9478.0, 9478.0, 4811.0, 146.0, 5.0, 4523.0, 'South Korea')\n",
      "('2020-03-30', 14829.0, 14829.0, 1595.0, 753.0, 36.0, 12934.0, 'Switzerland')\n",
      "('2020-03-30', 6280.0, 6280.0, 573.0, 625.0, 5.0, 5642.0, 'Canada')\n",
      "('2020-03-30', 1866.0, 1866.0, 424.0, 173.0, 2.0, 1388.0, 'Japan')\n",
      "('2020-03-30', 62095.0, 62095.0, 9211.0, 4400.0, 108.0, 52343.0, 'Germany')\n",
      "('2020-03-30', 4163.0, 4163.0, 226.0, 528.0, 3.0, 3920.0, 'Australia')\n",
      "('2020-03-30', 475.0, 475.0, 6.0, 119.0, 1.0, 459.0, 'United Kingdom')\n",
      "('2020-03-30', 141812.0, 141812.0, 4435.0, 18234.0, 255.0, 134902.0, 'United States')\n",
      "('2020-03-30', 3700.0, 3700.0, 16.0, 253.0, 5.0, 3574.0, 'Sweden')\n",
      "('2020-03-30', 10866.0, 10866.0, 250.0, 1104.0, 132.0, 9845.0, 'Netherlands')\n",
      "('2020-03-30', 4284.0, 4284.0, 7.0, 269.0, 2.0, 4252.0, 'Norway')\n",
      "('2020-03-30', 514.0, 514.0, 56.0, None, None, None, None)\n",
      "('2020-03-30', 40174.0, 40174.0, 7202.0, 2599.0, 292.0, 30366.0, 'France')\n",
      "('2020-03-30', 2395.0, 2395.0, 1.0, 194.0, 7.0, 2322.0, 'Denmark')\n",
      "('2020-03-30', 1240.0, 1240.0, 10.0, 73.0, 2.0, 1219.0, 'Finland')\n",
      "('2020-03-30', 81439.0, 81439.0, 75448.0, None, None, None, None)\n",
      "('2020-03-30', 844.0, 844.0, 212.0, 42.0, 1.0, 629.0, 'Singapore')\n",
      "('2020-03-30', 97689.0, 97689.0, 13030.0, 5217.0, 756.0, 73880.0, 'Italy')\n",
      "('2020-03-30', 8788.0, 8788.0, 479.0, 517.0, 18.0, 8223.0, 'Austria')\n",
      "('2020-03-30', 80110.0, 80110.0, 14709.0, 6875.0, 821.0, 58598.0, 'Spain')\n",
      "('2020-03-30', 9583.0, 9583.0, 5033.0, 105.0, 8.0, 4398.0, 'South Korea')\n",
      "('2020-04-01', 16605.0, 16605.0, 1823.0, 683.0, 74.0, 14349.0, 'Switzerland')\n",
      "('2020-04-01', 8612.0, 8612.0, 1242.0, 1164.0, 12.0, 7269.0, 'Canada')\n",
      "('2020-04-01', 1953.0, 1953.0, 424.0, None, None, 1473.0, 'Japan')\n",
      "('2020-04-01', 71808.0, 71808.0, 16100.0, 4923.0, 130.0, 54933.0, 'Germany')\n",
      "('2020-04-01', 4763.0, 4763.0, 345.0, 303.0, 1.0, 4398.0, 'Australia')\n",
      "('2020-04-01', 645.0, 645.0, 10.0, 97.0, 4.0, 618.0, 'United Kingdom')\n",
      "('2020-04-01', 187340.0, 187340.0, 6461.0, 23552.0, 719.0, 177019.0, 'United States')\n",
      "('2020-04-01', 4435.0, 4435.0, 16.0, 407.0, 34.0, 4239.0, 'Sweden')\n",
      "('2020-04-01', 12595.0, 12595.0, 250.0, 845.0, 175.0, 11306.0, 'Netherlands')\n",
      "('2020-04-01', 4641.0, 4641.0, 13.0, 196.0, 7.0, 4589.0, 'Norway')\n",
      "('2020-04-01', 647.0, 647.0, 74.0, 58.0, None, 572.0, 'New Zealand')\n",
      "('2020-04-01', 52128.0, 52128.0, 9444.0, 7578.0, 499.0, 39161.0, 'France')\n",
      "('2020-04-01', 2860.0, 2860.0, 1.0, 283.0, 13.0, 2769.0, 'Denmark')\n",
      "('2020-04-01', 1418.0, 1418.0, 10.0, 66.0, 4.0, 1391.0, 'Finland')\n",
      "('2020-04-01', 81518.0, 81518.0, 76052.0, None, None, 2161.0, 'China')\n",
      "('2020-04-01', 926.0, 926.0, 240.0, 47.0, None, 683.0, 'Singapore')\n",
      "('2020-04-01', 105792.0, 105792.0, 15729.0, 4053.0, 837.0, 77635.0, 'Italy')\n",
      "('2020-04-01', 10180.0, 10180.0, 1095.0, 562.0, 20.0, 8957.0, 'Austria')\n",
      "('2020-04-01', 95923.0, 95923.0, 19259.0, 7967.0, 748.0, 68200.0, 'Spain')\n",
      "('2020-04-01', 9786.0, 9786.0, 5408.0, 125.0, 4.0, 4216.0, 'South Korea')\n",
      "('2020-04-05', 21100.0, 21100.0, 6415.0, 595.0, 19.0, 14000.0, 'Switzerland')\n",
      "('2020-04-05', 14018.0, 14018.0, 2603.0, 106.0, 2.0, 11182.0, 'Canada')\n",
      "('2020-04-05', 3139.0, 3139.0, 514.0, None, None, 2548.0, 'Japan')\n",
      "('2020-04-05', 97074.0, 97074.0, 26400.0, 982.0, 34.0, 69196.0, 'Germany')\n",
      "('2020-04-05', 5687.0, 5687.0, 2315.0, 137.0, 4.0, 3338.0, 'Australia')\n",
      "('2020-04-05', 47806.0, 47806.0, 135.0, 5903.0, 619.0, 42739.0, 'United Kingdom')\n",
      "('2020-04-05', 311656.0, 311656.0, 14828.0, 299.0, 2.0, 288374.0, 'United States')\n",
      "('2020-04-05', 6830.0, 6830.0, 205.0, 387.0, 28.0, 6224.0, 'Sweden')\n",
      "('2020-04-05', 17851.0, 17851.0, 250.0, 1224.0, 115.0, 15835.0, 'Netherlands')\n",
      "('2020-04-05', 5645.0, 5645.0, 32.0, 95.0, 4.0, 5547.0, 'Norway')\n",
      "('2020-04-05', 1039.0, 1039.0, 156.0, 89.0, None, 882.0, 'New Zealand')\n",
      "('2020-04-05', 89953.0, 89953.0, 15438.0, None, None, 66955.0, 'France')\n",
      "('2020-04-05', 4369.0, 4369.0, 1327.0, 292.0, 18.0, 2863.0, 'Denmark')\n",
      "('2020-04-05', 1927.0, 1927.0, 300.0, 45.0, 3.0, 1599.0, 'Finland')\n",
      "('2020-04-05', 81669.0, 81669.0, 76964.0, 30.0, 3.0, 1376.0, 'China')\n",
      "('2020-04-05', 1309.0, 1309.0, 297.0, 120.0, None, 1006.0, 'Singapore')\n",
      "('2020-04-05', 124632.0, 124632.0, 20996.0, None, None, 88274.0, 'Italy')\n",
      "('2020-04-05', 11920.0, 11920.0, 2998.0, 139.0, 18.0, 8718.0, 'Austria')\n",
      "('2020-04-05', 130759.0, 130759.0, 38080.0, 4591.0, 471.0, 80261.0, 'Spain')\n",
      "('2020-04-05', 10237.0, 10237.0, 6463.0, 81.0, 6.0, 3591.0, 'South Korea')\n",
      "('2020-04-07', 21657.0, 21657.0, 8056.0, 557.0, 50.0, 12836.0, 'Switzerland')\n",
      "('2020-04-07', 16667.0, 16667.0, 3616.0, 1155.0, 43.0, 12728.0, 'Canada')\n",
      "('2020-04-07', 3654.0, 3654.0, 575.0, None, None, 2994.0, 'Japan')\n",
      "('2020-04-07', 103374.0, 103374.0, 28700.0, 3251.0, 226.0, 72864.0, 'Germany')\n",
      "('2020-04-07', 5895.0, 5895.0, 2432.0, 145.0, 8.0, 3418.0, 'Australia')\n",
      "('2020-04-07', 51608.0, 51608.0, 135.0, 3802.0, 439.0, 46100.0, 'United Kingdom')\n",
      "('2020-04-07', 366112.0, 366112.0, 19573.0, 29439.0, 1243.0, 335680.0, 'United States')\n",
      "('2020-04-07', 7206.0, 7206.0, 205.0, 376.0, 76.0, 6524.0, 'Sweden')\n",
      "('2020-04-07', 18803.0, 18803.0, 250.0, 952.0, 101.0, 16686.0, 'Netherlands')\n",
      "('2020-04-07', 5865.0, 5865.0, 32.0, 178.0, 5.0, 5757.0, 'Norway')\n",
      "('2020-04-07', 1106.0, 1106.0, 176.0, 67.0, None, 929.0, 'New Zealand')\n",
      "('2020-04-07', 98010.0, 98010.0, 17250.0, 5171.0, 833.0, 71849.0, 'France')\n",
      "('2020-04-07', 4681.0, 4681.0, 1378.0, 312.0, 8.0, 3116.0, 'Denmark')\n",
      "('2020-04-07', 2176.0, 2176.0, 300.0, 249.0, None, 1849.0, 'Finland')\n",
      "('2020-04-07', 81708.0, 81708.0, 77078.0, None, None, 1299.0, 'China')\n",
      "('2020-04-07', 1375.0, 1375.0, 344.0, 66.0, None, 1025.0, 'Singapore')\n",
      "('2020-04-07', 132547.0, 132547.0, 22837.0, 3599.0, 636.0, 93187.0, 'Italy')\n",
      "('2020-04-07', 12297.0, 12297.0, 3463.0, 246.0, 16.0, 8614.0, 'Austria')\n",
      "('2020-04-07', 136675.0, 136675.0, 40437.0, 5029.0, 700.0, 82897.0, 'Spain')\n",
      "('2020-04-07', 10284.0, 10284.0, 6598.0, 47.0, 3.0, 3500.0, 'South Korea')\n",
      "('2020-04-11', 24551.0, 1002.0, 11100.0, 500.0, 54.0, 12449.0, 'Switzerland')\n",
      "('2020-04-11', 22148.0, 569.0, 6013.0, 1383.0, 60.0, 15566.0, 'Canada')\n",
      "('2020-04-11', 5530.0, 99.0, 685.0, 183.0, None, 4746.0, 'Japan')\n",
      "('2020-04-11', 122171.0, 2767.0, 53913.0, 3936.0, 160.0, 65491.0, 'Germany')\n",
      "('2020-04-11', 6238.0, 54.0, 3141.0, 86.0, 3.0, 3043.0, 'Australia')\n",
      "('2020-04-11', 73758.0, 8958.0, 344.0, 8681.0, 980.0, 64456.0, 'United Kingdom')\n",
      "('2020-04-11', 501880.0, 18699.0, 27239.0, 33314.0, 2017.0, 455942.0, 'United States')\n",
      "('2020-04-11', 9685.0, 870.0, 381.0, 544.0, 77.0, 8434.0, 'Sweden')\n",
      "('2020-04-11', 23097.0, 2511.0, 250.0, 1335.0, 115.0, 20336.0, 'Netherlands')\n",
      "('2020-04-11', 6314.0, 113.0, 32.0, 95.0, 5.0, 6169.0, 'Norway')\n",
      "('2020-04-11', 1283.0, 2.0, 373.0, 44.0, 1.0, 908.0, 'New Zealand')\n",
      "('2020-04-11', 124869.0, 13197.0, 24932.0, 7120.0, 987.0, 86740.0, 'France')\n",
      "('2020-04-11', 5819.0, 247.0, 1773.0, 184.0, 10.0, 3799.0, 'Denmark')\n",
      "('2020-04-11', 2769.0, 48.0, 300.0, 164.0, 6.0, 2421.0, 'Finland')\n",
      "('2020-04-11', 81907.0, 3336.0, 77455.0, None, None, 1116.0, 'China')\n",
      "('2020-04-11', 2108.0, 7.0, 492.0, 198.0, 1.0, 1609.0, 'Singapore')\n",
      "('2020-04-11', 147577.0, 18849.0, 30455.0, 3951.0, 570.0, 98273.0, 'Italy')\n",
      "('2020-04-11', 13560.0, 319.0, 6064.0, 316.0, 24.0, 7177.0, 'Austria')\n",
      "('2020-04-11', 158273.0, 16081.0, 55668.0, 5051.0, 634.0, 86524.0, 'Spain')\n",
      "('2020-04-11', 10450.0, 208.0, 7117.0, 27.0, 4.0, 3125.0, 'South Korea')\n",
      "('2020-04-17', 26732.0, 1281.0, 15900.0, 396.0, 42.0, 9551.0, 'Switzerland')\n",
      "('2020-04-17', 29929.0, 1191.0, 9674.0, 1550.0, 181.0, 19064.0, 'Canada')\n",
      "('2020-04-17', 8626.0, 178.0, 901.0, None, None, 7547.0, 'Japan')\n",
      "('2020-04-17', 137698.0, 4052.0, 77000.0, 2945.0, 248.0, 56646.0, 'Germany')\n",
      "('2020-04-17', 6468.0, 63.0, 3747.0, 21.0, None, 2658.0, 'Australia')\n",
      "('2020-04-17', 103093.0, 13729.0, None, 4617.0, 861.0, 89020.0, 'United Kingdom')\n",
      "('2020-04-17', 677056.0, 34580.0, 57271.0, 29053.0, 2137.0, 585205.0, 'United States')\n",
      "('2020-04-17', 12540.0, 1333.0, 550.0, 613.0, 130.0, 10657.0, 'Sweden')\n",
      "('2020-04-17', 29214.0, 3315.0, 250.0, 1061.0, 181.0, 25649.0, 'Netherlands')\n",
      "('2020-04-17', 6896.0, 152.0, 32.0, 99.0, 2.0, 6712.0, 'Norway')\n",
      "('2020-04-17', 1401.0, 9.0, 770.0, 15.0, None, 622.0, 'New Zealand')\n",
      "('2020-04-17', 165027.0, 17920.0, 32812.0, 17164.0, 753.0, 114295.0, 'France')\n",
      "('2020-04-17', 6879.0, 321.0, 3023.0, 198.0, 12.0, 3535.0, 'Denmark')\n",
      "('2020-04-17', 3369.0, 75.0, 1700.0, 132.0, 3.0, 1594.0, 'Finland')\n",
      "('2020-04-17', 82341.0, 3342.0, 77892.0, None, None, 1107.0, 'China')\n",
      "('2020-04-17', 4427.0, 10.0, 683.0, 728.0, None, 3734.0, 'Singapore')\n",
      "('2020-04-17', 168941.0, 22170.0, 40164.0, 3786.0, 525.0, 106607.0, 'Italy')\n",
      "('2020-04-17', 14476.0, 410.0, 8986.0, 126.0, 17.0, 5080.0, 'Austria')\n",
      "('2020-04-17', 184948.0, 19315.0, 74797.0, 4289.0, 503.0, 90836.0, 'Spain')\n",
      "('2020-04-17', 10613.0, 229.0, 7757.0, 22.0, 4.0, 2627.0, 'South Korea')\n",
      "('2020-04-18', 27078.0, 1327.0, 16400.0, 346.0, 46.0, 9351.0, 'Switzerland')\n",
      "('2020-04-18', 31642.0, 1310.0, 10328.0, 1536.0, 115.0, 20004.0, 'Canada')\n",
      "('2020-04-18', 9787.0, 190.0, 935.0, 556.0, None, 8662.0, 'Japan')\n",
      "('2020-04-18', 141397.0, 4352.0, 83114.0, 3699.0, 300.0, 53931.0, 'Germany')\n",
      "('2020-04-18', 6526.0, 65.0, 3821.0, 58.0, 2.0, 2640.0, 'Australia')\n",
      "('2020-04-18', 108692.0, 14576.0, None, 5599.0, 847.0, 93772.0, 'United Kingdom')\n",
      "('2020-04-18', 709201.0, 37135.0, 59997.0, 31631.0, 2516.0, 612069.0, 'United States')\n",
      "('2020-04-18', 13216.0, 1400.0, 550.0, 676.0, 67.0, 11266.0, 'Sweden')\n",
      "('2020-04-18', 30449.0, 3459.0, 250.0, 1235.0, 144.0, 26740.0, 'Netherlands')\n",
      "('2020-04-18', 6937.0, 161.0, 32.0, 32.0, 9.0, 6744.0, 'Norway')\n",
      "('2020-04-18', 1409.0, 11.0, 816.0, 8.0, 2.0, 582.0, 'New Zealand')\n",
      "('2020-04-18', 147969.0, 18681.0, 34420.0, 1909.0, 761.0, 94868.0, 'France')\n",
      "('2020-04-18', 7073.0, 336.0, 3389.0, 194.0, 15.0, 3348.0, 'Denmark')\n",
      "('2020-04-18', 3489.0, 82.0, 1700.0, 120.0, 7.0, 1707.0, 'Finland')\n",
      "('2020-04-18', 82692.0, 4632.0, 77944.0, 325.0, 1290.0, 116.0, 'China')\n",
      "('2020-04-18', 5050.0, 11.0, 708.0, 623.0, 1.0, 4331.0, 'Singapore')\n",
      "('2020-04-18', 172434.0, 22745.0, 42727.0, 3493.0, 575.0, 106962.0, 'Italy')\n",
      "('2020-04-18', 14595.0, 431.0, 9704.0, 119.0, 21.0, 4460.0, 'Austria')\n",
      "('2020-04-18', 190839.0, 20002.0, 74797.0, 5891.0, 687.0, 96040.0, 'Spain')\n",
      "('2020-04-18', 10635.0, 230.0, 7829.0, 22.0, 1.0, 2576.0, 'South Korea')\n"
     ]
    }
   ],
   "source": [
    "class COVIDCases:  \n",
    "    #refit a dictionary to the 73 most influential countries\n",
    "    def fit_list(self, dictionary): \n",
    "        \"\"\"\n",
    "        Takes in a dictionary, returns a dictionary with corrected key names. \n",
    "        \"\"\"\n",
    "        returndict = {}\n",
    "        #make the following edits: Czechia -> Czech Republic, S.Korea -> South Korea, UAE  -> United Arab Emirates, UK -> United Kingdom, USA -> United States \n",
    "        deletelist = ['Czechia', 'S. Korea', 'UAE', 'UK', 'USA']\n",
    "        countryfixlist = ['Czech Republic', 'South Korea', 'United Arab Emirates', 'United Kingdom', 'United States']\n",
    "        datafixlist = []\n",
    "\n",
    "        #grab corresponding values for the keys that you are about to delete, create datafixlist\n",
    "        for country in deletelist: \n",
    "            datafixlist.append(dictionary[country]) \n",
    "        #delete all wrongly named countries \n",
    "        for country in deletelist: \n",
    "            del dictionary[country]\n",
    "        #reappend the correct key value pairs with correct country nomenclature \n",
    "        for i in range(len(datafixlist)): \n",
    "            dictionary[countryfixlist[i]] = datafixlist[i]\n",
    "        #remove all countries not considered influential \n",
    "        for country in influential_countries:\n",
    "            returndict[country] = dictionary[country]\n",
    "        return returndict\n",
    "\n",
    "    \n",
    "    #get the country: case dictionary for all 73 influential countries \n",
    "    def get_country_cases_dict(self): \n",
    "        \"\"\"\n",
    "        Returns a dictionary for 73 most influential countries and corresponding number of cases. \n",
    "        \"\"\"\n",
    "        url = \"https://coronavirus-monitor.p.rapidapi.com/coronavirus/cases_by_country.php\"\n",
    "        headers = {\n",
    "            'x-rapidapi-host': \"coronavirus-monitor.p.rapidapi.com\",\n",
    "            'x-rapidapi-key': \"c902dd2b74mshf8b7588307400dap1d10c3jsnd41616838035\"\n",
    "            }\n",
    "\n",
    "        country_cases = requests.request(\"GET\", url, headers=headers).json() \n",
    "\n",
    "        final_country_cases = {}\n",
    "\n",
    "        for i in range(len(country_cases['countries_stat'])): \n",
    "            #make a final dictionary with country_name -> number of cases \n",
    "            final_country_cases[country_cases['countries_stat'][i]['country_name']] = float(country_cases['countries_stat'][i]['cases'].replace(',','').strip()) \n",
    "        final_country_cases = self.fit_list(final_country_cases)\n",
    "        return final_country_cases\n",
    "\n",
    "\n",
    "    def get_covid_info_by_country_today(self, country): \n",
    "        \"\"\"\n",
    "        Takes in country, returns COVID data for that country.  \n",
    "        \"\"\"\n",
    "        table = {}\n",
    "        final_dict = {}\n",
    "        listofdicts = []\n",
    "\n",
    "        url = \"https://coronavirus-monitor.p.rapidapi.com/coronavirus/history_by_particular_country_by_date.php\"\n",
    "\n",
    "        headers = {\n",
    "        'x-rapidapi-host': \"coronavirus-monitor.p.rapidapi.com\",\n",
    "        'x-rapidapi-key': \"c902dd2b74mshf8b7588307400dap1d10c3jsnd41616838035\"\n",
    "        }\n",
    "\n",
    "        querystring = {\"country\": country, \"date\": today}\n",
    "\n",
    "        response = requests.request(\"GET\", url, headers=headers, params=querystring).json() \n",
    "        \n",
    "        #need to loop through and average by date. need to make date look prettier.\n",
    "        table['Date'] = today\n",
    "        try: \n",
    "            table['Total Cases'] = float(response['stat_by_country'][0]['total_cases'].replace(',',''))\n",
    "        except: \n",
    "            pass\n",
    "        try: \n",
    "            table['Total Deaths'] = float(response['stat_by_country'][0]['total_deaths'].replace(',',''))\n",
    "        except: \n",
    "            pass \n",
    "        try: \n",
    "            table['Total Recovered'] = float(response['stat_by_country'][0]['total_recovered'].replace(',',''))\n",
    "        except: \n",
    "            pass \n",
    "        try: \n",
    "            table['New Cases'] = float(response['stat_by_country'][0]['new_cases'].replace(',',''))\n",
    "        except: \n",
    "            pass\n",
    "        try: \n",
    "            table['New Deaths'] = float(response['stat_by_country'][0]['new_deaths'].replace(',',''))\n",
    "        except:\n",
    "            pass\n",
    "        try: \n",
    "            table['Active Cases'] = float(response['stat_by_country'][0]['active_cases'].replace(',',''))\n",
    "        except: \n",
    "            pass \n",
    "        try: \n",
    "            table['Country'] = country\n",
    "        except: \n",
    "            pass \n",
    "\n",
    "        return table \n",
    "    \n",
    "    def create_and_insert_cases_tuple(self): \n",
    "        \"\"\"\n",
    "        Creates/returns the tuple for 20 most influential countries to insert into database. \n",
    "        \"\"\"\n",
    "        dataframe = []\n",
    "        #convert to a dataframe\n",
    "        for country in top20forfinalcountrycases: \n",
    "            dataframe.append(self.get_covid_info_by_country_today(country))\n",
    "        #create dataframe \n",
    "        casesdf = pd.DataFrame(dataframe)  \n",
    "        #replace the names to be consistent with the full nomenclature \n",
    "        try: \n",
    "            casesdf[\"Country\"]= casesdf[\"Country\"].replace('USA', \"United States\") \n",
    "        except: \n",
    "            pass\n",
    "        try: \n",
    "            casesdf[\"Country\"]= casesdf[\"Country\"].replace('S. Korea', \"South Korea\") \n",
    "        except: \n",
    "            pass\n",
    "        try: \n",
    "            casesdf[\"Country\"]= casesdf[\"Country\"].replace('UK', \"United Kingdom\") \n",
    "        except:\n",
    "            pass \n",
    "        #create insertcasestuple \n",
    "        insertcasestuple = list(map(tuple, casesdf.to_numpy()))\n",
    "        return insertcasestuple \n",
    "\n",
    "\n",
    "    #run everytime you want to create/append \n",
    "    def create_and_append_cases(self, insertcasestuple): \n",
    "        \"\"\"\n",
    "        Take in tuple, append data to cases table in data database. \n",
    "        \"\"\"\n",
    "        #connect/create database\n",
    "        conn = sqlite3.connect('data.db')\n",
    "        conn.execute('''CREATE TABLE if not exists cases (date text, total_cases float, total_deaths float, total_recovered float, new_cases float, new_deaths float, active_cases float, country text)''')\n",
    "        conn.executemany('INSERT INTO cases VALUES (?,?,?,?,?,?,?,?)', insertcasestuple)\n",
    "        #print output  \n",
    "        for row in conn.execute('SELECT * FROM cases'):\n",
    "            print(row)\n",
    "\n",
    "        conn.commit() \n",
    "        conn.close() \n",
    "\n",
    "\n",
    "# UN-COMMENT BELOW TO ADD CASES RUN THIS ONCE A DAY! FOR 5 days! Takes 5-10 minutes to run due to latency from API. \n",
    "casesobj = COVIDCases() \n",
    "casesobj.create_and_append_cases(casesobj.create_and_insert_cases_tuple())\n",
    "# UN-COMMENT BELOW IF YOU HAVE A LOT OF TIME TO WAIT. \n",
    "# final_country_cases = casesobj.get_country_cases_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-5614bf6c8f73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;31m#UN-COMMENT below if you would like to add different stock's data for the last 20 days.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m \u001b[0mstockinput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Enter a stock ticker: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0mstock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstockinput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0mstock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_and_append_stocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_last_20_days_stock_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    864\u001b[0m         )\n\u001b[1;32m    865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    891\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 893\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    894\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "class Stocks: \n",
    "    #API KEY: M2OPINUK58M3497T\n",
    "    #https://robinhood.com/collections/100-most-popular\n",
    "\n",
    "    def __init__(self, tickerlist): \n",
    "        \"\"\"\n",
    "        Initializes Stock object. Creates past_20_days attribute, a list of the past 20 dates. \n",
    "        \"\"\"\n",
    "        self.tickerlist = tickerlist \n",
    "        self.past_20_days = self.get_past_20_days()\n",
    "    def get_past_20_days(self): \n",
    "        \"\"\"\n",
    "        Return a list of the past 20 dates \n",
    "        \"\"\"\n",
    "        past_20_days = []\n",
    "        today = DT.date.today()\n",
    "\n",
    "        for i in range(1,21): \n",
    "            day_ago = today - DT.timedelta(days=i)\n",
    "            past_20_days.append(str(day_ago).split('date')[0]) \n",
    "\n",
    "        return past_20_days \n",
    "\n",
    "\n",
    "    def get_last_20_days_stock_data(self):  \n",
    "        \"\"\"\n",
    "        Uses the stock object attribute to create a request URL for the past 20 dates, return a list of tuples with that stock\n",
    "        for the past 20 days. \n",
    "        \"\"\"\n",
    "        listoftuples = []\n",
    "        for ticker in self.tickerlist: \n",
    "            url = 'https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol='+ ticker + '&outputsize=compact&apikey=M2OPINUK58M3497T'\n",
    "            response = requests.get(url).json() \n",
    "\n",
    "            for date in self.past_20_days: \n",
    "                try: \n",
    "                    listoftuples.append((date, ticker, float(response['Time Series (Daily)'][date]['4. close']))) \n",
    "                except: \n",
    "                    continue \n",
    "\n",
    "        return listoftuples \n",
    "    \n",
    "    #run everytime you want to create/append \n",
    "    def create_and_append_stocks(self, inserttuple): \n",
    "        \"\"\"\n",
    "        Take in a tuple, append the data inside the tuple into a stocks table. \n",
    "        \"\"\"\n",
    "        #connect/create database\n",
    "        conn = sqlite3.connect('data.db')\n",
    "        conn.execute('''CREATE TABLE if not exists stocks (date text, ticker text, price float)''')\n",
    "        conn.executemany('INSERT INTO stocks VALUES (?,?,?)', inserttuple)\n",
    "        #print output  \n",
    "        for row in conn.execute('SELECT * FROM stocks'):\n",
    "            print(row)\n",
    "\n",
    "        conn.commit() \n",
    "        conn.close() \n",
    "\n",
    "#UN-COMMENT below if you would like to add different stock's data for the last 20 days. \n",
    "\n",
    "stockinput = input('Enter a stock ticker: ')\n",
    "stock = Stocks([stockinput])\n",
    "stock.create_and_append_stocks(stock.get_last_20_days_stock_data())  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Music Data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a 1,2,3,4,5 (to choose what list of 20 countries) : 1\n",
      "Artist Data: [('2020-04-18', 'Norway', 'Coldplay', 5521938.0, 'rock'), ('2020-04-18', 'Switzerland', 'Coldplay', 5521938.0, 'rock'), ('2020-04-18', 'Australia', 'Radiohead', 4856426.0, 'alternative'), ('2020-04-18', 'Ireland', 'David Bowie', 3477542.0, 'rock'), ('2020-04-18', 'Germany', 'Queen', 4177952.0, 'classic rock'), ('2020-04-18', 'Iceland', 'David Bowie', 3477542.0, 'rock'), ('2020-04-18', 'Sweden', 'David Bowie', 3477542.0, 'rock'), ('2020-04-18', 'Hong Kong', 'Coldplay', 5521938.0, 'rock'), ('2020-04-18', 'Singapore', 'Coldplay', 5521938.0, 'rock'), ('2020-04-18', 'Netherlands', 'Coldplay', 5521938.0, 'rock'), ('2020-04-18', 'Denmark', 'Queen', 4177952.0, 'classic rock'), ('2020-04-18', 'Canada', 'Radiohead', 4856426.0, 'alternative'), ('2020-04-18', 'United States', 'Radiohead', 4856426.0, 'alternative'), ('2020-04-18', 'United Kingdom', 'David Bowie', 3477542.0, 'rock'), ('2020-04-18', 'Finland', 'Queen', 4177952.0, 'classic rock'), ('2020-04-18', 'New Zealand', 'Radiohead', 4856426.0, 'alternative'), ('2020-04-18', 'Belgium', 'David Bowie', 3477542.0, 'rock'), ('2020-04-18', 'Liechtenstein', 'Metallica', 2984954.0, 'thrash metal'), ('2020-04-18', 'Japan', '宇多田ヒカル', 386972.0, 'j-pop'), ('2020-04-18', 'Austria', 'Queen', 4177952.0, 'classic rock')]\n",
      "Track Data: [('2020-04-18', 'Norway', 'Hello', 295.0, 451730.0, 'Adele', 'soul'), ('2020-04-18', 'Switzerland', 'Hello', 295.0, 451730.0, 'Adele', 'soul'), ('2020-04-18', 'Australia', 'The Less I Know the Better', 216.0, 555234.0, 'Tame Impala', 'psychedelic'), ('2020-04-18', 'Ireland', 'Africa', 295.0, 1011259.0, 'Toto', '80s'), ('2020-04-18', 'Germany', 'Smells Like Teen Spirit', 287.0, 2149211.0, 'Nirvana', 'Grunge'), ('2020-04-18', 'Iceland', 'The Less I Know the Better', 216.0, 555234.0, 'Tame Impala', 'psychedelic'), ('2020-04-18', 'Sweden', 'Hello', 295.0, 451730.0, 'Adele', 'soul'), ('2020-04-18', 'Hong Kong', 'Hello', 295.0, 451730.0, 'Adele', 'soul'), ('2020-04-18', 'Singapore', 'Hello', 295.0, 451730.0, 'Adele', 'soul'), ('2020-04-18', 'Netherlands', 'Africa', 295.0, 1011259.0, 'Toto', '80s'), ('2020-04-18', 'Denmark', '7 Years', 237.0, 285354.0, 'Lukas Graham', 'Worst Song Ever'), ('2020-04-18', 'Canada', \"Can't Feel My Face\", 216.0, 526502.0, 'The Weeknd', 'love at first listen'), ('2020-04-18', 'United States', \"Can't Feel My Face\", 216.0, 526502.0, 'The Weeknd', 'love at first listen'), ('2020-04-18', 'United Kingdom', 'Mr. Brightside', 193.0, 2067775.0, 'The Killers', 'rock'), ('2020-04-18', 'Finland', 'Africa', 295.0, 1011259.0, 'Toto', '80s'), ('2020-04-18', 'New Zealand', 'Mr. Brightside', 193.0, 2067775.0, 'The Killers', 'rock'), ('2020-04-18', 'Belgium', 'The Less I Know the Better', 216.0, 555234.0, 'Tame Impala', 'psychedelic'), ('2020-04-18', 'Liechtenstein', 'Smells Like Teen Spirit', 287.0, 2149211.0, 'Nirvana', 'Grunge'), ('2020-04-18', 'Japan', 'ロビンソン', 261.0, 22472.0, 'スピッツ', 'japanese'), ('2020-04-18', 'Austria', 'Mr. Brightside', 193.0, 2067775.0, 'The Killers', 'rock')]\n"
     ]
    }
   ],
   "source": [
    "top100 = ['Norway', 'Switzerland','Australia','Ireland','Germany','Iceland','Sweden','Hong Kong','Singapore','Netherlands','Denmark','Canada','United States','United Kingdom','Finland','New Zealand','Belgium', 'Liechtenstein', 'Japan', 'Austria', 'Luxembourg', 'Haiti', 'Israel', 'France', 'Slovenia', 'Spain','Italy','Malta','Estonia', 'Greece', 'Cyprus', 'Poland', 'United Arab Emirates','Lithuania', 'Andorra', 'Qatar', 'Slovakia','Nepal', 'Saudi Arabia', 'Portugal', 'Latvia', 'Bahrain', 'Chile', 'Hungary', 'Croatia', 'Argentina', 'Oman', 'Chad', 'Montenegro', 'Bulgaria', 'Romania', 'Belarus', 'Bahamas', 'Uruguay', 'Kuwait', 'Malaysia', 'Barbados', 'Kazakhstan', 'Palau', 'Angola', 'Seychelles', 'Costa Rica', 'Turkey', 'Mauritius', 'Panama', 'Serbia', 'Albania', 'Trinidad and Tobago', 'Antigua and Barbuda', 'Georgia', 'Saint Kitts and Nevis', 'Cuba', 'Mexico', 'Grenada', 'Sri Lanka', 'Bosnia and Herzegovina', 'Venezuela', 'Brazil', 'Lebanon', 'Azerbaijan', 'Thailand', 'Armenia', 'Algeria', 'Ecuador', 'China', 'Ukraine', 'Peru', 'Saint Lucia', 'Colombia', 'Mongolia', 'Fiji', 'Dominican Republic', 'Tunisia', 'Jordan', 'Jamaica', 'Tonga', 'Saint Vincent and the Grenadines', 'Suriname', 'Botswana', 'Maldives']\n",
    "twentyone = []\n",
    "twentytwo = []\n",
    "twentythree = []\n",
    "twentyfour = []\n",
    "twentyfive = []\n",
    "\n",
    "twentyone = top100[0:20]\n",
    "\n",
    "twentytwo= top100[20:40]\n",
    "\n",
    "twentythree= top100[40:60]\n",
    "\n",
    "twentyfour= top100[60:80]\n",
    "\n",
    "twentyfive= top100[80:100]\n",
    "\n",
    "\n",
    "\n",
    "#take in user input 1-5 to grab different sets of data with manually changing the code each run. \n",
    "userinput = input('Enter a 1,2,3,4,5 (to choose what list of 20 countries) : ')\n",
    "\n",
    "\n",
    "APIKEY = '010c0f8122235fc473fbb31980871052'\n",
    "def request_url_geo_top_artists(country):\n",
    "    \"\"\"\n",
    "    Pass in the country. Returns request url for geo top artists API endpoint. \n",
    "    \"\"\"\n",
    "    url = 'http://ws.audioscrobbler.com/2.0/?method=geo.gettopartists&country=' + country + '&api_key=' + APIKEY + '&format=json'\n",
    "    return url \n",
    "\n",
    "\n",
    "def request_url_geo_top_tracks(country): \n",
    "    \"\"\"\n",
    "    Pass in the country. Returns request url for geo top tracks API endpoint. \n",
    "    \"\"\"\n",
    "    url = 'http://ws.audioscrobbler.com/2.0/?method=geo.gettoptracks&country=' + country + '&api_key=' + APIKEY + '&format=json'\n",
    "    return url \n",
    "\n",
    "\n",
    "\n",
    "def get_top_artists(request_url): \n",
    "    \"\"\"\n",
    "    Pass in a request url. Returns a list of tuples to prepare data for database and visualization. \n",
    "    \"\"\"    \n",
    "    data = requests.get(request_url).json()\n",
    "    \n",
    "    country = request_url.split('&')[1].split('=')[1]\n",
    "    \n",
    "    \n",
    "    artist = data['topartists']['artist'][0]['name']\n",
    "    listeners = float(data['topartists']['artist'][0]['listeners']) \n",
    "    artistdata = requests.get('http://ws.audioscrobbler.com/2.0/?method=artist.getinfo&artist=' + artist + '&api_key=010c0f8122235fc473fbb31980871052&format=json').json() \n",
    "    genre = artistdata['artist']['tags']['tag'][0]['name']\n",
    "    \n",
    "    tuplelist = [(today, country, artist, listeners, genre)]\n",
    "    return tuplelist \n",
    "\n",
    "def get_top_tracks(request_url): \n",
    "    \"\"\"\n",
    "    Pass in a request url. Returns a list of tuples to prepare data for database and visualization. \n",
    "    \"\"\"\n",
    "    data = requests.get(request_url).json()\n",
    "    country = request_url.split('&')[1].split('=')[1]\n",
    "    trackname = data['tracks']['track'][0]['name'] \n",
    "    listeners = float(data['tracks']['track'][0]['listeners']) \n",
    "    artist = data['tracks']['track'][0]['artist']['name']\n",
    "    genredata = requests.get('http://ws.audioscrobbler.com/2.0/?method=track.getInfo&api_key=010c0f8122235fc473fbb31980871052&artist=' + artist + '&track=' + trackname + '&format=json').json()\n",
    "    genre = genredata['track']['toptags']['tag'][0]['name']\n",
    "    duration = float(genredata['track']['duration'])/1000\n",
    "    tuplelist = [(today, country, trackname, duration, listeners, artist, genre)]    \n",
    "    return tuplelist    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "#list of urls \n",
    "urllist = [] \n",
    "urllist2 = []\n",
    "\n",
    "\n",
    "#first list of countries 1-20 \n",
    "if int(userinput) == 1:\n",
    "    \n",
    "     \n",
    "    artist_data = []\n",
    "    track_data = []\n",
    "\n",
    "    \n",
    "    #make a list of all the request urls\n",
    "    for country in twentyone: \n",
    "        urllist.append(request_url_geo_top_artists(country)) \n",
    "        \n",
    "    for url in urllist: \n",
    "        artist_data += get_top_artists(url)\n",
    "    \n",
    "    \n",
    "    #make a list of all the request urls\n",
    "    for country in twentyone: \n",
    "        urllist2.append(request_url_geo_top_tracks(country))\n",
    "    \n",
    "    \n",
    "    #grab the list of one tuple. add each entry to a final list \n",
    "    for url in urllist2: \n",
    "        track_data += get_top_tracks(url) \n",
    "    \n",
    "    \n",
    "    print('Artist Data: ' + str(artist_data))\n",
    "    print('Track Data: ' + str(track_data))  \n",
    "    \n",
    "#second list of countries 21-40 \n",
    "elif int(userinput) == 2:\n",
    "    \n",
    "    artist_data = []\n",
    "    track_data = []\n",
    "\n",
    "    #make a list of all the request urls\n",
    "    for country in twentytwo: \n",
    "        urllist.append(request_url_geo_top_artists(country))\n",
    "    for url in urllist: \n",
    "        artist_data += get_top_artists(url)\n",
    "        \n",
    "    #make a list of all the request urls\n",
    "    for country in twentytwo: \n",
    "        urllist2.append(request_url_geo_top_tracks(country))\n",
    "    \n",
    "    \n",
    "    #grab the list of one tuple. add each entry to a final list \n",
    "    for url in urllist2: \n",
    "        track_data += get_top_tracks(url) \n",
    "        \n",
    "    print('Artist Data: ' + str(artist_data))\n",
    "    print('Track Data: ' + str(track_data))  \n",
    "    \n",
    "#third list of countries 41-60\n",
    "elif int(userinput) == 3: \n",
    "    \n",
    "    artist_data = []\n",
    "    track_data = []\n",
    "\n",
    "    #make a list of all the request urls\n",
    "    for country in twentythree: \n",
    "        urllist.append(request_url_geo_top_artists(country))\n",
    "        \n",
    "    for url in urllist: \n",
    "        artist_data += get_top_artists(url)\n",
    "    \n",
    "    #make a list of all the request urls\n",
    "    for country in twentythree: \n",
    "        urllist2.append(request_url_geo_top_tracks(country))\n",
    "    \n",
    "    \n",
    "    #grab the list of one tuple. add each entry to a final list \n",
    "    for url in urllist2: \n",
    "        track_data += get_top_tracks(url) \n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    print('Artist Data: ' + str(artist_data))\n",
    "    print('Track Data: ' + str(track_data))  \n",
    "    \n",
    "    \n",
    "#4th list of countries 61-80\n",
    "elif int(userinput) == 4: \n",
    "    \n",
    "    artist_data = []\n",
    "    track_data = []\n",
    "\n",
    "    #make a list of all the request urls\n",
    "    for country in twentyfour:\n",
    "        urllist.append(request_url_geo_top_artists(country))\n",
    "    for url in urllist: \n",
    "        artist_data += get_top_artists(url)\n",
    "        \n",
    "        \n",
    "    #make a list of all the request urls\n",
    "    for country in twentyfour: \n",
    "        urllist2.append(request_url_geo_top_tracks(country))\n",
    "    \n",
    "    \n",
    "    #grab the list of one tuple. add each entry to a final list \n",
    "    for url in urllist2: \n",
    "        track_data += get_top_tracks(url) \n",
    "        \n",
    "        \n",
    "    print('Artist Data: ' + str(artist_data))\n",
    "    print('Track Data: ' + str(track_data))  \n",
    "    \n",
    "\n",
    "#5th list of countries 81-100 \n",
    "elif int(userinput) == 5: \n",
    "    \n",
    "    artist_data = []\n",
    "    track_data = []\n",
    "\n",
    "    #make a list of all the request urls\n",
    "    for country in twentyfive:\n",
    "        urllist.append(request_url_geo_top_artists(country))\n",
    "    \n",
    "    #grab the list of one tuple. add each entry to a final list \n",
    "    for url in urllist: \n",
    "        artist_data += get_top_artists(url)\n",
    "        \n",
    "            \n",
    "    #make a list of all the request urls\n",
    "    for country in twentyfive: \n",
    "        urllist2.append(request_url_geo_top_tracks(country))\n",
    "    \n",
    "    \n",
    "    #grab the list of one tuple. add each entry to a final list \n",
    "    for url in urllist2: \n",
    "        track_data += get_top_tracks(url) \n",
    "        \n",
    "    print('Artist Data: ' + str(artist_data))\n",
    "    print('Track Data: ' + str(track_data))  \n",
    "\n",
    "    \n",
    "#run everytime you want to create/append \n",
    "def create_and_append_song_data(): \n",
    "    \"\"\"\n",
    "    Creates tracks & artists table if they don't exist. Append data to tracks and artists table. Returns nothing. \n",
    "    \"\"\"\n",
    "    #connect/create database\n",
    "    conn = sqlite3.connect('data.db')\n",
    "    \n",
    "    #creating and inserting into top artists table \n",
    "\n",
    "    conn.execute('''CREATE TABLE IF NOT EXISTS artists(date text, country text, artist text, listeners float, genre text)''')\n",
    "    conn.executemany('INSERT INTO artists VALUES (?,?,?,?,?)', artist_data)\n",
    "    \n",
    "    #creating and inserting into top tracks table \n",
    "    conn.execute('''CREATE TABLE IF NOT EXISTS tracks(date text, country text, track text, duration float, listeners float, artist text, genre text)''')\n",
    "    conn.executemany('INSERT INTO tracks VALUES (?,?,?,?,?,?,?)', track_data)\n",
    "    \n",
    "    #both the artists and tracks data share a joint key of the date and the country. \n",
    "    \n",
    "    #print output  \n",
    "    for row in conn.execute('SELECT * FROM tracks'):\n",
    "        print(row)\n",
    "    for row in conn.execute('SELECT * FROM artists'):\n",
    "        print(row)\n",
    "    \n",
    "\n",
    "    conn.commit() \n",
    "    conn.close() \n",
    "\n",
    "\n",
    "#UN-COMMENT BELOW IF YOU WOULD LIKE TO APPEND SONG DATA \n",
    "# create_and_append_song_data() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Querying, Calculating, & Visualizations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/olili/.local/lib/python3.6/site-packages/matplotlib/backends/_backend_pdf_ps.py:62: RuntimeWarning: Glyph 23431 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=ft2font.LOAD_NO_HINTING)\n",
      "/home/olili/.local/lib/python3.6/site-packages/matplotlib/backends/_backend_pdf_ps.py:62: RuntimeWarning: Glyph 22810 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=ft2font.LOAD_NO_HINTING)\n",
      "/home/olili/.local/lib/python3.6/site-packages/matplotlib/backends/_backend_pdf_ps.py:62: RuntimeWarning: Glyph 30000 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=ft2font.LOAD_NO_HINTING)\n",
      "/home/olili/.local/lib/python3.6/site-packages/matplotlib/backends/_backend_pdf_ps.py:62: RuntimeWarning: Glyph 12498 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=ft2font.LOAD_NO_HINTING)\n",
      "/home/olili/.local/lib/python3.6/site-packages/matplotlib/backends/_backend_pdf_ps.py:62: RuntimeWarning: Glyph 12459 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=ft2font.LOAD_NO_HINTING)\n",
      "/home/olili/.local/lib/python3.6/site-packages/matplotlib/backends/_backend_pdf_ps.py:62: RuntimeWarning: Glyph 12523 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=ft2font.LOAD_NO_HINTING)\n",
      "/home/olili/.local/lib/python3.6/site-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 23431 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/home/olili/.local/lib/python3.6/site-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 22810 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/home/olili/.local/lib/python3.6/site-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 30000 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/home/olili/.local/lib/python3.6/site-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12498 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/home/olili/.local/lib/python3.6/site-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12459 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/home/olili/.local/lib/python3.6/site-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12523 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/home/olili/.local/lib/python3.6/site-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 23431 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/home/olili/.local/lib/python3.6/site-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 22810 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/home/olili/.local/lib/python3.6/site-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 30000 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/home/olili/.local/lib/python3.6/site-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12498 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/home/olili/.local/lib/python3.6/site-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12459 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/home/olili/.local/lib/python3.6/site-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12523 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Cases Table[(140,)]\n",
      "Length of Stocks Table[(196,)]\n",
      "Length of Weather Table[(140,)]\n",
      "Length of Tracks Table[(100,)]\n",
      "Length of Artists Table[(100,)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2520x2160 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2160x2160 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x1800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Calculate: \n",
    "\n",
    "\n",
    "    def avg_temp_by_country(self): \n",
    "        \"\"\"\n",
    "        Calculates the average temperature by country. Returns a tuple that contains the query result. \n",
    "        \"\"\"\n",
    "        result = []\n",
    "        conn = sqlite3.connect('data.db')\n",
    "        for row in conn.execute('''SELECT AVG(Fahrenheit) as Avg_Temp_F, country FROM country_weather WHERE country <> 'None' GROUP BY country'''):\n",
    "            result.append(row)\n",
    "        conn.commit() \n",
    "        conn.close()\n",
    "        return result \n",
    "    \n",
    "    def historic_cases(self): \n",
    "        \"\"\"\n",
    "        Calculates the cases by date and country. Returns a tuple that contains the query result. \n",
    "        \"\"\"\n",
    "        result = []\n",
    "        conn = sqlite3.connect('data.db')\n",
    "        for row in conn.execute('''SELECT date, country, MAX(total_cases) FROM cases WHERE country <> 'None' GROUP BY date, country'''):\n",
    "            result.append(row)\n",
    "        conn.commit() \n",
    "        conn.close()\n",
    "        return result \n",
    "    \n",
    "    \n",
    "    def avg_new_cases_per_day_by_country(self):\n",
    "        \"\"\"\n",
    "        Calculates the average new cases by country. Returns a tuple that contains the query result. \n",
    "        \"\"\"\n",
    "        result = []\n",
    "        conn = sqlite3.connect('data.db')\n",
    "        for row in conn.execute('''SELECT country, AVG(new_cases) FROM cases WHERE country <> 'None' GROUP BY country'''):\n",
    "            result.append(row)\n",
    "        conn.commit() \n",
    "        conn.close()\n",
    "        return result \n",
    "    \n",
    "\n",
    "    def join_calc(self): \n",
    "        \"\"\"\n",
    "        Calculates using a JOIN the average weather, total number of cases. Returns a tuple that contains the query result. \n",
    "        \"\"\"\n",
    "        result = []\n",
    "        conn = sqlite3.connect('data.db')\n",
    "        for row in conn.execute('''SELECT AVG(Fahrenheit) as Avg_Temp, MAX(total_cases) as total_cases, w.country FROM country_weather w INNER JOIN cases c on w.country == c.country WHERE w.country <> 'None' GROUP by w.country'''):\n",
    "            result.append(row) \n",
    "        conn.commit() \n",
    "        conn.close()\n",
    "        return result \n",
    "    def avg_stock_price(self): \n",
    "        \"\"\"\n",
    "        Calculates average stock price per stock. Returns a tuple that contains the query result. \n",
    "        \"\"\"\n",
    "        result = []\n",
    "        conn = sqlite3.connect('data.db')\n",
    "        for row in conn.execute('''SELECT ticker, AVG(price) FROM stocks GROUP BY ticker'''):\n",
    "            result.append(row) \n",
    "        conn.commit() \n",
    "        conn.close()\n",
    "        return result \n",
    "    \n",
    "    \n",
    "    def stocks(self): \n",
    "        \"\"\"\n",
    "        Select all data from the stocks table. \n",
    "        \"\"\"\n",
    "        result = []\n",
    "        conn = sqlite3.connect('data.db')\n",
    "        for row in conn.execute('''SELECT * FROM stocks'''):\n",
    "            result.append(row) \n",
    "        conn.commit() \n",
    "        conn.close()\n",
    "        return result\n",
    "    \n",
    "    def check_length_cases(self): \n",
    "        \"\"\"\n",
    "        Checks the number of entries of the cases table. \n",
    "        \"\"\"\n",
    "        result = []\n",
    "        conn = sqlite3.connect('data.db')\n",
    "        for row in conn.execute('''SELECT COUNT(date) FROM cases'''):\n",
    "            result.append(row) \n",
    "        conn.commit() \n",
    "        conn.close()\n",
    "        return result \n",
    "    def check_length_stocks(self): \n",
    "        \"\"\"\n",
    "        Checks the number of entries of the stocks table. \n",
    "        \"\"\"\n",
    "        result = []\n",
    "        conn = sqlite3.connect('data.db')\n",
    "        for row in conn.execute('''SELECT COUNT(date) FROM stocks'''):\n",
    "            result.append(row) \n",
    "        conn.commit() \n",
    "        conn.close()\n",
    "        return result\n",
    "\n",
    "    def check_length_weather(self): \n",
    "        \"\"\"\n",
    "        Checks the number of entries of the country_weather table. \n",
    "        \"\"\"\n",
    "        result = []\n",
    "        conn = sqlite3.connect('data.db')\n",
    "        for row in conn.execute('''SELECT COUNT(date) FROM country_weather'''):\n",
    "            result.append(row) \n",
    "        conn.commit() \n",
    "        conn.close()\n",
    "        return result\n",
    "    def check_length_tracks(self): \n",
    "        \"\"\"\n",
    "        Checks the number of entries of the tracks table. \n",
    "        \"\"\"\n",
    "        result = []\n",
    "        conn = sqlite3.connect('data.db')\n",
    "        for row in conn.execute('''SELECT COUNT(date) FROM tracks'''):\n",
    "            result.append(row) \n",
    "        conn.commit() \n",
    "        conn.close()\n",
    "        return result\n",
    "    \n",
    "    def check_length_artists(self): \n",
    "        \"\"\"\n",
    "        Checks the number of entries of the artists table. \n",
    "        \"\"\"\n",
    "        result = []\n",
    "        conn = sqlite3.connect('data.db')\n",
    "        for row in conn.execute('''SELECT COUNT(date) FROM artists'''):\n",
    "            result.append(row) \n",
    "        conn.commit() \n",
    "        conn.close()\n",
    "        return result\n",
    "    \n",
    "\n",
    "    def artist_calculate(self): \n",
    "        \"\"\"\n",
    "        Returns the artist, and number of times that artist shows up \n",
    "        \"\"\"\n",
    "        result = []\n",
    "        conn = sqlite3.connect('data.db')\n",
    "\n",
    "        #count of the number of artists \n",
    "        for row in conn.execute('''SELECT artist, count(artist) FROM artists GROUP BY artist ORDER BY count(artist) desc'''):\n",
    "            result.append(row) \n",
    "        conn.commit() \n",
    "        conn.close()\n",
    "        return result\n",
    "\n",
    "\n",
    "    def track_calculate(self): \n",
    "        \"\"\"\n",
    "        Returns country, cases, # listeners, artist, and genre \n",
    "        \"\"\"\n",
    "        result = []\n",
    "        conn = sqlite3.connect('data.db')\n",
    "\n",
    "        #count of the number of artists \n",
    "        for row in conn.execute('''SELECT c.country, max(total_cases), max(listeners), artist, genre FROM artists a inner join cases c on a.country = c.country WHERE c.country <> 'None' GROUP by c.country '''):\n",
    "            result.append(row) \n",
    "        conn.commit() \n",
    "        conn.close()\n",
    "        return result\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "#create Calculate object named calc  \n",
    "calc = Calculate() \n",
    "\n",
    "calc.drop() \n",
    "\n",
    "#calculate the artist and the number of times the artist appears \n",
    "artistcount = calc.artist_calculate() \n",
    "\n",
    "\n",
    "#create artist dataframe  \n",
    "artistcountdf = pd.DataFrame.from_records(artistcount, columns = ['Artist', 'Artist Count'])\n",
    "\n",
    "#grab the cases, # listeners, artist and genre\n",
    "trackcount = calc.track_calculate() \n",
    "\n",
    "#create track dataframe  \n",
    "trackcountdf = pd.DataFrame.from_records(trackcount, columns = ['Country', '# of COVID-19 Cases', '# of Listeners',  'Artist', 'Genre'])\n",
    "\n",
    "#create scatter plot of number of listeners, number of covid-19 cases, and genre \n",
    "sns.scatterplot(x=\"# of Listeners\", y=\"# of COVID-19 Cases\", hue = 'Genre', data=trackcountdf)\n",
    "plt.tight_layout()\n",
    "plt.savefig('numcases_vs_listeners_by_genre.pdf')\n",
    "plt.clf() \n",
    "\n",
    "\n",
    "#create artist count visualization\n",
    "a4_dims = (35, 30)\n",
    "fig, ax = plt.subplots(figsize=a4_dims)\n",
    "sns.set(style=\"whitegrid\")\n",
    "sns.barplot(ax = ax, x=artistcountdf['Artist'], y=artistcountdf['Artist Count'])\n",
    "plt.savefig('topartistcount.pdf')\n",
    "plt.clf() \n",
    "\n",
    "\n",
    "\n",
    "#create dataframes for above queries \n",
    "joindf = pd.DataFrame.from_records(calc.join_calc(), columns =['Average Temperature of Different Countries (F)', 'Current # of Cases COVID-19', 'Country']) \n",
    "hcasesdf = pd.DataFrame.from_records(calc.historic_cases(), columns = ['Date', 'Country', 'Total # of COVID-19 Cases'])\n",
    "newcasesdf = pd.DataFrame.from_records(calc.avg_new_cases_per_day_by_country(), columns = ['Country', 'Average # of New COVID-19 Cases'])\n",
    "stocksdf = pd.DataFrame.from_records(calc.avg_stock_price(), columns = ['Ticker', 'Average Price'])\n",
    "\n",
    "\n",
    "sns.distplot(stocksdf['Average Price'])\n",
    "plt.savefig('average_stock_price.pdf')\n",
    "plt.clf() \n",
    "\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "a4_dims = (30, 30)\n",
    "fig, ax = plt.subplots(figsize=a4_dims)\n",
    "\n",
    "# Plot the responses for different events and regions\n",
    "sns.barplot(ax = ax, x=hcasesdf[\"Country\"], y=hcasesdf[\"Total # of COVID-19 Cases\"])\n",
    "plt.savefig('histogram_cases.pdf')\n",
    "plt.clf()\n",
    "\n",
    "# Plot the responses for different events and regions\n",
    "sns.barplot(x=newcasesdf[\"Country\"], y=newcasesdf[\"Average # of New COVID-19 Cases\"])\n",
    "plt.savefig('histogram_new_cases.pdf')\n",
    "plt.clf()\n",
    "\n",
    "#graph time series for stocks \n",
    "a4_dims = (25, 25)\n",
    "fig, ax = plt.subplots(figsize=a4_dims)\n",
    "stocktimeseriesdf = pd.DataFrame.from_records(calc.stocks(), columns =['Date', 'Ticker', 'Price']) \n",
    "sns.lineplot(ax = ax, x=stocktimeseriesdf[\"Date\"], y=stocktimeseriesdf['Price'], hue = stocktimeseriesdf['Ticker'])\n",
    "plt.savefig('stock_time_series.pdf')\n",
    "plt.clf() \n",
    "\n",
    "\n",
    "# Avg Temp vs. Current # Deaths \n",
    "sns.regplot(x=joindf[\"Average Temperature of Different Countries (F)\"], y=joindf[\"Current # of Cases COVID-19\"], color = 'green') \n",
    "plt.savefig('avg_temp_num_cases.pdf')\n",
    "plt.clf()\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "#write out avg_temp_by_country calculation to text/csv file                                \n",
    "with open('avg_temp_by_country.csv','w') as out:\n",
    "    csv_out = csv.writer(out)\n",
    "    csv_out.writerow(['Temperature (F)','Country'])\n",
    "    for row in calc.avg_temp_by_country():\n",
    "        csv_out.writerow(row)\n",
    "\n",
    "#write out temperature vs deaths COVID-19 calculation to text/csv file                                \n",
    "with open('temp_vs_cases.csv','w') as out:\n",
    "    csv_out = csv.writer(out)\n",
    "    csv_out.writerow(['Average Temperature (F)','Current # of Cases COVID-19', 'Country'])\n",
    "    for row in calc.join_calc(): \n",
    "        csv_out.writerow(row)\n",
    "\n",
    "#write out average number of new cases \n",
    "with open('avg_new_cases.csv','w') as out:\n",
    "    csv_out = csv.writer(out)\n",
    "    csv_out.writerow(['Country','Average # of New COVID-19 Cases'])\n",
    "    for row in calc.avg_new_cases_per_day_by_country(): \n",
    "        csv_out.writerow(row)\n",
    "    \n",
    "#write out average stock price per ticker chosen \n",
    "with open('avg_stock_price.csv','w') as out:\n",
    "    csv_out = csv.writer(out)\n",
    "    csv_out.writerow(['Ticker','Average Stock Price'])\n",
    "    for row in calc.avg_stock_price(): \n",
    "        csv_out.writerow(row)\n",
    "\n",
    "\n",
    "\n",
    "#write out covid-19 vs. listeners per genre  \n",
    "with open('numcases_vs_listeners_by_genre.csv','w') as out:\n",
    "    csv_out = csv.writer(out)\n",
    "    csv_out.writerow(['Country', '# of COVID-19 Cases', '# of Listeners',  'Artist', 'Genre'])\n",
    "    for row in trackcount: \n",
    "        csv_out.writerow(row)\n",
    "\n",
    "\n",
    "#write out artist and corresponding count \n",
    "with open('artistcount.csv','w') as out:\n",
    "    csv_out = csv.writer(out)\n",
    "    csv_out.writerow(['Artist','Artist Count'])\n",
    "    for row in artistcount: \n",
    "        csv_out.writerow(row)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#check the length of all the tables to check for 100 items. they all have at least 100 items. \n",
    "print('Length of Cases Table' + str(calc.check_length_cases()))  \n",
    "print('Length of Stocks Table' + str(calc.check_length_stocks())) \n",
    "print('Length of Weather Table' + str(calc.check_length_weather())) \n",
    "print('Length of Tracks Table' + str(calc.check_length_tracks())) \n",
    "print('Length of Artists Table' + str(calc.check_length_artists())) \n",
    "\n",
    "\n",
    "\n",
    "# #BELOW contains the pair plot and the correlation graph with continents. \n",
    "#if you have a lot of time to wait around, \n",
    "#you can un-comment the COVID-19 cases line way above. this creates two insightful charts attached in the report. \n",
    "\n",
    "\n",
    "# data = []\n",
    "# clist = []\n",
    "\n",
    "# #make the columns the country so its visible \n",
    "# clist = sorted(list(final_country_weather.keys()))  \n",
    "    \n",
    "# #make list of lists to ready data for pandas\n",
    "# for country in final_country_weather.keys(): \n",
    "#     #weather, cases, classification\n",
    "#     data.append([final_country_weather[country],final_country_cases[country], classif[country]]) \n",
    "    \n",
    "\n",
    "# #load into pandas \n",
    "# continentdf = pd.DataFrame(data, columns = ['Weather (F)', 'Current # of COVID-19 Cases', 'Continent']) \n",
    "\n",
    "# sns.set_style('white') \n",
    "# sns.set_style('ticks')\n",
    "\n",
    "# #graph the plots \n",
    "# lm = sns.lmplot(x= 'Weather (F)', y = 'Current # of COVID-19 Cases', hue = 'Continent', data = continentdf, height=5, aspect=1.6)\n",
    "# axes = lm.axes\n",
    "\n",
    "# #set the limit to make the graph look nicer \n",
    "# axes[0,0].set_ylim(0,50000)\n",
    "# plt.savefig('Weather_vs_Cases_regression.pdf')\n",
    "# plt.clf() \n",
    "\n",
    "\n",
    "# #graph pair plots of cases vs weather \n",
    "# pair = sns.pairplot(continentdf, hue = 'Continent', height = 4, aspect = 1.5) \n",
    "# axespair = pair.axes\n",
    "# axespair[1,1].set_xlim(0,10000)\n",
    "# plt.savefig('Weather_vs_Cases_pair_plot.pdf')\n",
    "# plt.clf()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
